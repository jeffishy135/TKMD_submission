import numpy as npfrom matplotlib import pyplot as pltimport scipy.statsfrom scipy.optimize import fmin_cobylafrom scipy.optimize import fminimport mathfrom multiprocessing import Poolimport timeimport syssys.setrecursionlimit(1000000)""" Calibration Functions"""# https://stats.stackexchange.com/questions/6022/estimating-a-distribution-based-on-three-percentiles# https://www.codeproject.com/Articles/56371/Finding-Probability-Distribution-Parameters-from-Pdef lognormal_parameters(x1, x2, x3, init_s, init_loc=0, init_scale=1):    """    Minimizing the L2 norm of quantile value calculations to generate distribution parameters    Args:            param1: the 25th quantile value            param2: the 50th quantile value            param3: the 75th quantile value            param4: initial value of s, shape parameter of log-normal dist.            param5: initial value of location for log-normal dist.            param6: initial value of scale for log-normal dist.    Returns:            The function estimates the parameters c, d for the Burr Distribution.    """    # Ensure that x1< x2< x3; they correspond to 25th, 50th, 75 percentiles respectively    verify = sorted([x1, x2, x3])    if verify!= [x1, x2, x3]:        raise ValueError("Check input quantiles")    # Objective function to be minimized    def objective(v):        s, loc, scale= v        distance = (scipy.stats.lognorm.ppf(.25, s, loc=loc, scale=scale) - x1)**2        distance += (scipy.stats.lognorm.ppf(.50, s, loc=loc, scale=scale) - x2)**2        distance += (scipy.stats.lognorm.ppf(.75, s, loc=loc, scale=scale) - x3)**2        return(distance)    # Location parameter cannot be negative    def constraint_loc(v):        return(v[1])    # Scale parameter cannot be negative    def constraint_scale(v):        return (v[2])    # Use COBYLA (Constrained optimization by linear approximation) method to minimize, random initialization at  all    xopt = fmin_cobyla(objective, np.array([init_s, init_loc, init_scale]), [constraint_loc, constraint_scale],                       rhoend=1e-4, disp=False)    return(xopt[0], xopt[1], xopt[2])def lognorm_sampling(logn_s, logn_loc, logn_scale, N):    """    Gives me a list of samples in each quartile    Args:            param1:  the shape parameter of lognormal Dist. s            param2:  the shape parameter of Burr Dist. location            param3:  the shape parameter of Burr Dist. scale            param4: the number of sample size    Returns:            Lists of values sampled (must be integer values).    """    sample =[int(x) for x in scipy.stats.lognorm.rvs(logn_s, loc=logn_loc, scale=logn_scale, size=N).tolist()]    return(sample)def beta_parameters(x1, x2, x3, alpha, beta):    """    Minimizing the L2 norm of quantile value calculations to generate distribution parameters    Args:            param1: the 25th quantile value            param2: the 50th quantile value            param3: the 75th quantile value            param4: initial value of alpha, shape parameter of beta dist.            param5: initial value of beta, shape parameter of beta dist.    Returns:            The function estimates the parameters c, d for the Burr Distribution.    """    # Ensure that x1< x2< x3; they correspond to 25th, 50th, 75 percentiles respectively    verify = sorted([x1, x2, x3])    if verify!= [x1, x2, x3]:        raise ValueError("Check input quantiles")    # Objective function to be minimized    def objective(v):        alpha, beta= v        distance = (scipy.stats.beta.ppf(.25, alpha, beta) - x1)**2        distance += (scipy.stats.beta.ppf(.50, alpha, beta) - x2)**2        distance += (scipy.stats.beta.ppf(.75, alpha, beta) - x3)**2        return(distance)    # Use fmin    xopt = fmin(objective, np.array([alpha, beta]), disp=False)    return(xopt[0], xopt[1])def beta_sampling(alpha, beta, N):    return(scipy.stats.beta.rvs(alpha, beta, size=N))# Herfindahl only by changing the spread of beta distribution# Herfindahl  index obtained from World Bank Data -- https://wits.worldbank.org/CountryProfile/en/Country/BEL/StartYear/2013/EndYear/2017/Indicator/HH-MKT-CNCNTRTN-NDXdef herfindahl_disparameters(x):    """    Minimizing the L2 norm of the Herfindahl index    Args:            param1: the observed Herfindahl index    Returns:            The parameters for a lognormal distribution.    """    def objective(v):        a, b= v        # sample N values and calculate the Herfindahl that way        herfindahl_est = np.sum(np.square(np.random.beta(a, b,  size=N)))        distance = (herfindahl_est - x)**2        return(distance)    xopt = fmin(objective, np.array([10,10000]), xtol=1e-8, ftol=1e-8, disp=False) #******* Change initial val to 10    return(xopt[0], xopt[1])def firm_domesticDemandShare_sampling(herf, N):    """    Generates a list of domestic share for product k    :Args:        param1: true observed Herfindahl Index    Returns;        a list of N shares of firm. Each share is in betwen 0, 1    """    a,b = herfindahl_disparameters(herf)    return(scipy.stats.beta.rvs(a, b, size=N).tolist())"""Graphical analysis"""def generate_graph(allNodes, allUpstreams, allImports, alldfs):    # Generating edge list    graph_dict_fan = {}    for i in range(len(allNodes)):        print(i)        upstream_lst = np.random.choice(allNodes, allUpstreams[i]).tolist() #*****Must have add argument      replace=False        firm_firm_lst = beta_sampling(firmfirm_a, firmfirm_b, allUpstreams[i]).tolist()        graph_dict_fan[i] = [upstream_lst, firm_firm_lst]    edge_list =[]    for node in allNodes:        val = graph_dict_fan[node]        if not val[0]:            pass        else:            for domsupplier in val[0]:                edge_tup = (domsupplier, node, val[1][val[0].index(domsupplier)])                edge_list.append(edge_tup)    # Generating import list    import_dict= {}    for importer in allImports:        import_dict[importer] =alldfs[allImports.index(importer)]    return(allNodes, edge_list, import_dict)from TKMD1_calibrationGrph import GraphEconomy"""Data"""N=100000percImport = 0.19percExport = 0.12dom_sellers_25p = 19dom_sellers_50p = 33dom_sellers_75p = 55dom_buyer_25p = 2dom_buyer_50p = 9dom_buyer_75p = 34firm_firm_share_25p =0.0002firm_firm_share_50p =0.0012firm_firm_share_75p =0.0053dir_fs_25p =0.01dir_fs_50p =0.28dir_fs_75p =0.67total_fs_25p =0.24total_fs_50p =0.39total_fs_75p =0.55herfindahl = 0.07"""Calibration of Distributions"""# Calibrate dom_sellers; dist_N1current_time = time.time()domsell_s, domsell_loc, domsell_scale = lognormal_parameters(dom_sellers_25p, dom_sellers_50p, dom_sellers_75p, 1,                                                              0, 1)print(time.time()- current_time)# Calibrate dom_buyers; dist_N2current_time = time.time()dombuy_s, dombuy_loc, dombuy_scale = lognormal_parameters(dom_buyer_25p, dom_buyer_50p, dom_buyer_75p, 1,                                                              0, 1)print(time.time()- current_time)# Calibrate direct foreign share; dist_S1current_time = time.time()dfs_a, dfs_b = beta_parameters(dir_fs_25p, dir_fs_50p, dir_fs_75p, 1, 1)print(time.time()- current_time)# Calibrate total firm-firm share; dist_S2current_time = time.time()firmfirm_a, firmfirm_b = beta_parameters(firm_firm_share_25p, firm_firm_share_50p, firm_firm_share_75p, 10, 1)print(time.time()- current_time)# Calibrate domestic firm product share; dist_S3current_time = time.time()firmproduct_a, firmproduct_b = herfindahl_disparameters(herfindahl)print(time.time()- current_time)# Calibrate total foreign share; dist_S4current_time = time.time()Tfs_a, Tfs_b = beta_parameters(total_fs_25p, total_fs_50p, total_fs_75p, 10, 1)print(time.time()- current_time)"""Generating Lists from Distributions"""# L1 = list of all firms - uniformly distributedL1_lst = [i for i in range(N)]# L2 = list of all importing firms - uniformly distributedL2_lst = sorted(np.random.randint(0, N+1, size=int(percImport*N)).tolist())# L3 = list of all exporting firms - uniformly distributedL3_lst = sorted(np.random.randint(0, N+1, size=int(percExport*N)).tolist())# P1 = List of number of sellers for each firm.. has length of NP1_sellers = lognorm_sampling(domsell_s, domsell_loc, domsell_scale, N)# P2 = List of  direct foreign shares..has length equal to L2_lstP2_dfs = beta_sampling(dfs_a, dfs_b, len(L2_lst))print("___begin long process___")"""Actual test case"""current_time = time.time()allfirms, edges, importers=generate_graph(L1_lst, P1_sellers, L2_lst, P2_dfs)test = [i for i in range(5)]print("done initializing")belgian_economy = GraphEconomy(importers)print("Adding edges")# build the graph through raw edgesbelgian_economy.addEdges(edges)print(len(belgian_economy.graph))