import numpy as npfrom matplotlib import pyplot as pltimport scipy.statsfrom scipy.optimize import fmin_cobylafrom scipy.optimize import fminimport scipy.sparseimport mathimport time""" Calibration Functions"""# https://stats.stackexchange.com/questions/6022/estimating-a-distribution-based-on-three-percentiles# https://www.codeproject.com/Articles/56371/Finding-Probability-Distribution-Parameters-from-Pdef lognormal_parameters(x1, x2, x3, init_s, init_loc=0, init_scale=1):    """    Minimizing the L2 norm of quantile value calculations to generate distribution parameters    Args:            param1: the 25th quantile value            param2: the 50th quantile value            param3: the 75th quantile value            param4: initial value of s, shape parameter of log-normal dist.            param5: initial value of location for log-normal dist.            param6: initial value of scale for log-normal dist.    Returns:            The function estimates the parameters c, d for the Burr Distribution.    """    # Ensure that x1< x2< x3; they correspond to 25th, 50th, 75 percentiles respectively    verify = sorted([x1, x2, x3])    if verify!= [x1, x2, x3]:        raise ValueError("Check input quantiles")    # Objective function to be minimized    def objective(v):        s, loc, scale= v        distance = (scipy.stats.lognorm.ppf(.25, s, loc=loc, scale=scale) - x1)**2        distance += (scipy.stats.lognorm.ppf(.50, s, loc=loc, scale=scale) - x2)**2        distance += (scipy.stats.lognorm.ppf(.75, s, loc=loc, scale=scale) - x3)**2        return(distance)    # Location parameter cannot be negative    def constraint_loc(v):        return(v[1])    # Scale parameter cannot be negative    def constraint_scale(v):        return (v[2])    # Use COBYLA (Constrained optimization by linear approximation) method to minimize, random initialization at  all    xopt = fmin_cobyla(objective, np.array([init_s, init_loc, init_scale]), [constraint_loc, constraint_scale],                       rhoend=1e-4, disp=False)    return(xopt[0], xopt[1], xopt[2])def lognorm_sampling(logn_s, logn_loc, logn_scale, N):    """    Gives me a list of samples in each quartile    Args:            param1:  the shape parameter of lognormal Dist. s            param2:  the shape parameter of Burr Dist. location            param3:  the shape parameter of Burr Dist. scale            param4: the number of sample size    Returns:            Lists of values sampled (must be integer values).    """    sample =[int(x) for x in scipy.stats.lognorm.rvs(logn_s, loc=logn_loc, scale=logn_scale, size=N).tolist()]    return(sample)def beta_parameters(x1, x2, x3, alpha, beta):    """    Minimizing the L2 norm of quantile value calculations to generate distribution parameters    Args:            param1: the 25th quantile value            param2: the 50th quantile value            param3: the 75th quantile value            param4: initial value of alpha, shape parameter of beta dist.            param5: initial value of beta, shape parameter of beta dist.    Returns:            The function estimates the parameters c, d for the Burr Distribution.    """    # Ensure that x1< x2< x3; they correspond to 25th, 50th, 75 percentiles respectively    verify = sorted([x1, x2, x3])    if verify!= [x1, x2, x3]:        raise ValueError("Check input quantiles")    # Objective function to be minimized    def objective(v):        alpha, beta= v        distance = (scipy.stats.beta.ppf(.25, alpha, beta) - x1)**2        distance += (scipy.stats.beta.ppf(.50, alpha, beta) - x2)**2        distance += (scipy.stats.beta.ppf(.75, alpha, beta) - x3)**2        return(distance)    # Use fmin    xopt = fmin(objective, np.array([alpha, beta]), disp=False)    return(xopt[0], xopt[1])def beta_sampling(alpha, beta, N):    return(scipy.stats.beta.rvs(alpha, beta, size=N))# Herfindahl only by changing the spread of beta distribution# Herfindahl  index obtained from World Bank Data -- https://wits.worldbank.org/CountryProfile/en/Country/BEL/StartYear/2013/EndYear/2017/Indicator/HH-MKT-CNCNTRTN-NDXdef herfindahl_disparameters(x):    """    Minimizing the L2 norm of the Herfindahl index    Args:            param1: the observed Herfindahl index    Returns:            The parameters for a lognormal distribution.    """    def objective(v):        a, b= v        # sample N values and calculate the Herfindahl that way        herfindahl_est = np.sum(np.square(np.random.beta(a, b,  size=N)))        distance = (herfindahl_est - x)**2        return(distance)    xopt = fmin(objective, np.array([100,60000]), xtol=1e-8, ftol=1e-8, disp=False) #******* Change initial val to 10    return(xopt[0], xopt[1])def firm_domesticDemandShare_sampling(herf, N):    """    Generates a list of domestic share for product k    :Args:        param1: true observed Herfindahl Index    Returns;        a list of N shares of firm. Each share is in betwen 0, 1    """    a,b = herfindahl_disparameters(herf)    return(scipy.stats.beta.rvs(a, b, size=N).tolist())"""Graphical analysis"""def generate_graph(allNodes, allUpstreams, allImports, alldfs):    # Generating edge list    graph_dict_fan = {}    for i in range(len(allNodes)):        upstream_lst = np.random.choice(allNodes, allUpstreams[i], replace=False).tolist()        firm_firm_lst = beta_sampling(firmfirm_a, firmfirm_b, allUpstreams[i]).tolist()        graph_dict_fan[i] = [upstream_lst, firm_firm_lst]    print(graph_dict_fan)    edge_list =[]    seller_lst = []    buyer_lst=[]    b2b_lst = []    for node in allNodes:        val = graph_dict_fan[node]        if not val[0]:            pass        else:            for domsupplier in val[0]:                edge_tup = (domsupplier, node, val[1][val[0].index(domsupplier)])                edge_list.append(edge_tup)            seller_lst.append(domsupplier)            buyer_lst.append(node)            b2b_lst.append(val[1][val[0].index(domsupplier)])    # Generating import list    import_dict= {}    for importer in allImports:        import_dict[importer] =alldfs[allImports.index(importer)]    return(allNodes, edge_list, import_dict, (seller_lst, buyer_lst, b2b_lst))"""Data"""N=100000percImport = 0.19percExport = 0.12dom_sellers_25p = 19dom_sellers_50p = 33dom_sellers_75p = 55dom_buyer_25p = 2dom_buyer_50p = 9dom_buyer_75p = 34firm_firm_share_25p =0.0002firm_firm_share_50p =0.0012firm_firm_share_75p =0.0053dir_fs_25p =0.01dir_fs_50p =0.28dir_fs_75p =0.67total_fs_25p =0.24total_fs_50p =0.39total_fs_75p =0.55labor_share_25p = 0.17labor_share_50p = 0.34labor_share_75p = 0.54herfindahl = 0.07sigma = 4rho = 2"""Calibration of Distributions"""# Calibrate dom_sellers; dist_N1domsell_s, domsell_loc, domsell_scale = lognormal_parameters(dom_sellers_25p, dom_sellers_50p, dom_sellers_75p, 1,                                                              0, 1)# Calibrate dom_buyers; dist_N2dombuy_s, dombuy_loc, dombuy_scale = lognormal_parameters(dom_buyer_25p, dom_buyer_50p, dom_buyer_75p, 1,                                                              0, 1)# Calibrate direct foreign share; dist_S1dfs_a, dfs_b = beta_parameters(dir_fs_25p, dir_fs_50p, dir_fs_75p, 1, 1)# Calibrate total firm-firm share; dist_S2firmfirm_a, firmfirm_b = beta_parameters(firm_firm_share_25p, firm_firm_share_50p, firm_firm_share_75p, 10, 1)# Calibrate domestic firm product share; dist_S3firmproduct_a, firmproduct_b = herfindahl_disparameters(herfindahl)# Calibrate total foreign share; dist_S4Tfs_a, Tfs_b = beta_parameters(total_fs_25p, total_fs_50p, total_fs_75p, 10, 1)# Calibrate labor share; dist_S4labor_a, labor_b = beta_parameters(labor_share_25p, labor_share_50p, labor_share_75p, 2, 1)"""Generating Lists from Distributions"""# L1 = list of all firms - uniformly distributedL1_lst = [i for i in range(N)]# L2 = list of all importing firms - uniformly distributedL2_lst = sorted(np.random.randint(0, N+1, size=int(percImport*N)).tolist())# L3 = list of all exporting firms - uniformly distributedL3_lst = sorted(np.random.randint(0, N+1, size=int(percExport*N)).tolist())# P1 = List of number of sellers for each firm.. has length of NP1_sellers = lognorm_sampling(domsell_s, domsell_loc, domsell_scale, N)# P2 = List of  direct foreign shares..has length equal to L2_lstP2_dfs = beta_sampling(dfs_a, dfs_b, len(L2_lst))# P3 = List of labor shareP3_labor = beta_sampling(labor_a, labor_b, N)"""Actual test case"""current_time = time.time()allfirms, edges, importers, edge_tup=generate_graph(L1_lst, P1_sellers, L2_lst, P2_dfs)firms_import = [0] * Nfor importer in L2_lst:    firms_import[importer] = P2_dfs[L2_lst.index(importer)]firms_import_share = np.transpose(np.array(firms_import)).reshape(N,1)firm_firm_sales = scipy.sparse.csr_matrix((edge_tup[2], (edge_tup[0],edge_tup[1])), shape = (N, N))# Row = supplier; column = buyer# total_inputs = firm_firm_sales.sum(axis=1) + firms_import_share + labor_share# firm_to_firm_input_share = firm_firm_sales / np.transpose(total_inputs)# print(firm_to_firm_input_share.shape)total_share_of_foreign_imports = scipy.sparse.linalg.gmres(scipy.sparse.identity(N) - np.transpose(firm_firm_sales),                                                           firms_import_share, restart=10, tol=1e-10)firm_TFS = total_share_of_foreign_imports[0].reshape(N,1)firm_dirFS = firms_import_sharedef wagesol_autarky(fs_i):    return(np.mean(np.power(np.sum([1.-fs_i,  ((1-sigma)/(1-rho))/N]), (1/(sigma-1)))))autarky_wage_tot = wagesol_autarky(firm_TFS)autarky_wage_dir = wagesol_autarky(firm_dirFS)print("Total real wage change to autarky: ", autarky_wage_dir)def wagesol_fshock(fs_i, perc):    weighted_value = (1-np.array(fs_i)+np.array(fs_i)*(1+perc)**(1-rho))**((1-sigma)/(1-rho))    return np.sum(weighted_value/N)**(1/(sigma-1))f10perc_wage = wagesol_fshock(firm_TFS, 0.1)print("Total real wage change to uniform foreign shock" , f10perc_wage)print("Non-target referece: ", np.mean(scipy.stats.entropy(firm_TFS, np.transpose(np.array(beta_sampling(Tfs_a, Tfs_b,                                                                                                     N))))))